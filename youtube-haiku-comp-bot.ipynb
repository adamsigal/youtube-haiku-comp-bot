{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Adam Sigal\n",
    "import praw\n",
    "from apiclient.discovery import build\n",
    "import datetime\n",
    "import isodate\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import math\n",
    "from pytube import YouTube\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey='AIzaSyDdkPJNXR6_ZoifmGWGTxknxpOjmOHmxgA')\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"3ad_76DYm9V_8w\",\n",
    "                     client_secret=\"_SfshD61Ht7L31b2y2FPSNkq-DY\",\n",
    "                     user_agent=\"my user agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:21:03\n"
     ]
    }
   ],
   "source": [
    "request = youtube.videos().list(\n",
    "    part=\"snippet,contentDetails\",\n",
    "    id=\"Ks-_Mh1QhMc\"\n",
    ")\n",
    "response = request.execute()\n",
    "title = response['items'][0]['snippet']['title']\n",
    "duration_str = response['items'][0]['contentDetails']['duration']\n",
    "duration_timedelta = isodate.parse_duration(duration_str)\n",
    "print(duration_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouts to Willem Van Onsem: \n",
    "# https://stackoverflow.com/questions/45579306/get-youtube-video-url-or-youtube-video-id-from-a-string-using-regex\n",
    "def get_yt_id(url):\n",
    "    u_pars = urlparse(url)\n",
    "    quer_v = parse_qs(u_pars.query).get('v')\n",
    "    if quer_v:\n",
    "        return quer_v[0]\n",
    "    pth = u_pars.path.split('/')\n",
    "    if pth:\n",
    "        return pth[-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: handle clipped videos correctly.\n",
    "# urls that start late and end normally end with ?t=85 (where 85 is #seconds)\n",
    "# clipped: https://www.youtube.com/embed/c_jomXhjUjI?start=2&end=32\n",
    "\n",
    "# period (str): \"week\", \"month\", \"year\", \"all time\"\n",
    "# time_limit (timedelta): time limit for a given compilation\n",
    "# max_vids (int): max number of vids for a given compilation\n",
    "# min_score (int): min # of upvotes to get into the compilation\n",
    "def get_submission_list(period=\"month\", time_limit=datetime.timedelta.max, max_vids=50, min_score=0):\n",
    "    submission_list = []\n",
    "    ctr = 0\n",
    "    total_duration = datetime.timedelta(seconds = 0)\n",
    "    for submission in reddit.subreddit(\"youtubehaiku\").top(period):\n",
    "        if (ctr >= max_vids):\n",
    "            #print(\"ctr value: \" + str(ctr))\n",
    "            break\n",
    "        try:\n",
    "            vid_id = get_yt_id(submission.url)\n",
    "\n",
    "            # To make sure that link isn't broken\n",
    "            request = youtube.videos().list(\n",
    "                part=\"ContentDetails\",\n",
    "                id=vid_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            # if 'items' is empty, no vid data was retrievable => don't want in list\n",
    "            assert len(response['items']) != 0\n",
    "\n",
    "            duration_str = response['items'][0]['contentDetails']['duration']\n",
    "            duration_timedelta = isodate.parse_duration(duration_str)\n",
    "            \n",
    "            \n",
    "            if (total_duration+duration_timedelta > time_limit) or (submission.score < min_score):\n",
    "                break\n",
    "            \n",
    "            submission_list.append(submission)\n",
    "            total_duration += duration_timedelta\n",
    "            ctr += 1\n",
    "            \n",
    "        except:\n",
    "            print(\"Error with post (probs dead link): '%s'. \\nurl: %s\\nContinuing...\" % (submission.title, submission.url))\n",
    "                \n",
    "    print(\"length of list: \" + str(len(submission_list)))\n",
    "    print(\"total duration: \" + str(total_duration))\n",
    "    return (submission_list, total_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of list: 29\n",
      "total duration: 0:09:27\n"
     ]
    }
   ],
   "source": [
    "#list = get_submission_list(time_limit=datetime.timedelta(minutes=15, seconds=46))\n",
    "list = get_submission_list(period=\"week\",  min_score=1000)\n",
    "\n",
    "# request = youtube.videos().list(\n",
    "#     part=\"ContentDetails\",\n",
    "#     id=\"t-ZRX8984sc\"\n",
    "# )\n",
    "# response = request.execute()\n",
    "# try:\n",
    "#     assert len(response['items']) != 0 , \"length is 0\"\n",
    "#     print(\"we good\")\n",
    "# except:\n",
    "#     print(\"nex thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might be obselete with total_duration included in get_submission_list()\n",
    "def get_total_duration(submissions):\n",
    "    total_duration = datetime.timedelta(seconds = 0)\n",
    "    \n",
    "    for submission in submissions:\n",
    "        vid_id = get_yt_id(submission.url)\n",
    "        request = youtube.videos().list(\n",
    "            part=\"ContentDetails\",\n",
    "            id=vid_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        duration_str = response['items'][0]['contentDetails']['duration']\n",
    "        duration_timedelta = isodate.parse_duration(duration_str)\n",
    "        \n",
    "        total_duration += duration_timedelta\n",
    "    \n",
    "    return total_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_URLS = [\n",
    "    ('iwGFalTRHDA', 'http://youtube.com/watch?v=iwGFalTRHDA'),\n",
    "    ('iwGFalTRHDA', 'http://www.youtube.com/watch?v=iwGFalTRHDA&feature=related'),\n",
    "    ('iwGFalTRHDA', 'https://youtube.com/iwGFalTRHDA'),\n",
    "    ('n17B_uFF4cA', 'http://youtu.be/n17B_uFF4cA'),\n",
    "    ('iwGFalTRHDA', 'youtube.com/iwGFalTRHDA'),\n",
    "    ('n17B_uFF4cA', 'youtube.com/n17B_uFF4cA'),\n",
    "    ('r5nB9u4jjy4', 'http://www.youtube.com/embed/watch?feature=player_embedded&v=r5nB9u4jjy4'),\n",
    "    ('t-ZRX8984sc', 'http://www.youtube.com/watch?v=t-ZRX8984sc'),\n",
    "    ('t-ZRX8984sc', 'http://youtu.be/t-ZRX8984sc'),\n",
    "    (None, 'http://www.stackoverflow.com')\n",
    "]\n",
    "\n",
    "def yt_id_tests(test_urls):\n",
    "    for (id, url) in test_urls:\n",
    "        try:\n",
    "            print(\"id: \" + id)\n",
    "            print(\"get_id: \" + get_id(url))\n",
    "            if id == get_id(url):\n",
    "                print(\"same\")\n",
    "                #print(\"IDs are same: \" + id + \" \" + get_id(url))\n",
    "            else:\n",
    "                print(\"different\")\n",
    "                #print(\"IDs are different: \" + id + \" \" + get_id(url))\n",
    "            print()\n",
    "        except:\n",
    "            print(\"An error occured. id: %s, url: %s. \\nContinuing...\" % (id, url))\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adam/Documents/compsci/youtube-haiku-comp-bot/./vids/cat.mp4'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: for highest quality (1080p and above) you need to merge \n",
    "# separate audio and video streams\n",
    "# https://python-pytube.readthedocs.io/en/latest/user/quickstart.html\n",
    "\n",
    "video = YouTube('https://www.youtube.com/watch?v=8p8P3VKd-Ko')\n",
    "video.streams.filter(file_extension = \"mp4\")\n",
    "\n",
    "# The HQ streams are called 'adaptive' and are called like so:\n",
    "video.streams.filter(adaptive=True)\n",
    "\n",
    "# The lower quality combined A/V streams are called 'progressive', & are called like so:\n",
    "streams = video.streams.filter(progressive=True)\n",
    "\n",
    "streams.get_highest_resolution().download('./vids', 'cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_vids(submissions):\n",
    "    for submission in submissions:\n",
    "        vid = YouTube(submission.url)\n",
    "        streams = vid.streams.filter(progressive=True)\n",
    "        streams.get_highest_resolution().download('./vids', str(submissions.index(submission) + 1) + \"-\" + submission.title.replace(\" \", \"_\"))\n",
    "        #print(str(submissions.index(submission) + 1) + \"-\" + submission.title.replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of list: 5\n",
      "total duration: 0:01:20\n"
     ]
    }
   ],
   "source": [
    "download_vids(get_submission_list(period=\"week\", max_vids=5)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
